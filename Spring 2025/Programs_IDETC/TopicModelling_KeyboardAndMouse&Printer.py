import osimport pandas as pdimport refrom bertopic import BERTopicfrom sentence_transformers import SentenceTransformerfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerfrom sklearn.decomposition import LatentDirichletAllocation, NMF# File pathfile_path = r"D:\OneDrive\Academic History\Research\NLP-Driven Sustainable Design_CoDesign Lab\Co-Design Lab\Databases_IDETC\V1 Merged with Product Group\filtered_data.csv"output_dir = r"D:\OneDrive\Academic History\Research\NLP-Driven Sustainable Design_CoDesign Lab\Co-Design Lab\Databases_IDETC\V1 Merged with Product Group\Topic_Outputs"# Ensure output directory existsos.makedirs(output_dir, exist_ok=True)# Load the datasetdf = pd.read_csv(file_path, encoding='ISO-8859-1')# Filter relevant product typestarget_product_types = {"Keyboard", "Keyboard and mouse", "Printer"}df_filtered = df[df["product_type"].isin(target_product_types)]# Preprocess text columndf_filtered["content"] = (    df_filtered["content"].fillna('')        .str.lower()        .str.replace(r'\W', ' ', regex=True)        .str.replace(r'\s+', ' ', regex=True))# Initialize BERTopic modelembedding_model = SentenceTransformer("all-MiniLM-L6-v2")topic_model = BERTopic(umap_model=None)# Function to run LDA Topic Modelingdef run_lda(texts, num_topics=5):    """Performs LDA Topic Modeling with dynamic df thresholds"""    if len(texts) < 5:        print("⚠️ Skipping LDA: Not enough documents for meaningful topics.")        return ["Not enough data for LDA."]    # Dynamically adjust min_df and max_df    min_df_value = 2 if len(texts) >= 10 else 1    max_df_value = 0.95 if len(texts) >= 10 else 0.8    # Initialize Vectorizer    vectorizer = CountVectorizer(max_df=max_df_value, min_df=min_df_value, stop_words='english')    try:        X = vectorizer.fit_transform(texts)    except ValueError:        print("⚠️ LDA skipped due to insufficient words after filtering.")        return ["LDA skipped due to insufficient word frequency."]    # Apply LDA    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)    lda.fit(X)    feature_names = vectorizer.get_feature_names_out()    topics = []    for idx, topic in enumerate(lda.components_):        topic_words = ", ".join([feature_names[i] for i in topic.argsort()[:-11:-1]])        topics.append(f"Topic {idx + 1}: {topic_words}")    return topics# Function to run NMF Topic Modelingdef run_nmf(texts, num_topics=5):    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)    X = vectorizer.fit_transform(texts)    nmf = NMF(n_components=num_topics, random_state=42)    nmf.fit(X)    feature_names = vectorizer.get_feature_names_out()    topics = []    for idx, topic in enumerate(nmf.components_):        topic_words = ", ".join([feature_names[i] for i in topic.argsort()[:-11:-1]])        topics.append(f"Topic {idx + 1}: {topic_words}")    return topics# Function to run BERTopicdef run_bertopic(texts):    """Performs BERTopic Modeling but skips if not enough data"""    if len(texts) < 2:        print("⚠️ Skipping BERTopic: Not enough documents for meaningful clustering.")        return ["Not enough data for BERTopic."]    # Generate embeddings    embeddings = embedding_model.encode(texts, show_progress_bar=False)    # Fit BERTopic model    try:        topics, _ = topic_model.fit_transform(texts, embeddings)    except ValueError:        print("⚠️ BERTopic skipped: Model cannot process a single sample.")        return ["BERTopic skipped due to single sample issue."]    topic_info = topic_model.get_topic_info()    topic_results = []    for _, row in topic_info.iterrows():        topic_num = row['Topic']        if topic_num != -1:  # Ignore outliers            top_words = topic_model.get_topic(topic_num)            words = ", ".join([word[0] for word in top_words])            topic_results.append(f"Topic {topic_num}: {words}")    return topic_results# Function to write results to txt filedef save_topics_to_txt(filename, lda_topics, nmf_topics, bertopic_topics):    file_path = os.path.join(output_dir, filename)    with open(file_path, "w", encoding="utf-8") as f:        f.write("========== LDA Topics ==========\n")        for topic in lda_topics:            f.write(topic + "\n")        f.write("\n========== NMF Topics ==========\n")        for topic in nmf_topics:            f.write(topic + "\n")        f.write("\n========== BERTopic Topics ==========\n")        for topic in bertopic_topics:            f.write(topic + "\n")    print(f"✅ Topics saved to: {file_path}")# Process by product typefor product_type in target_product_types:    df_product_type = df_filtered[df_filtered["product_type"] == product_type]    if not df_product_type.empty:        print(f"Processing {product_type} at product-type level...")        texts = df_product_type["content"].tolist()        lda_topics = run_lda(texts)        nmf_topics = run_nmf(texts)        bertopic_topics = run_bertopic(texts)        file_name = f"{product_type.replace(' ', '_')}_Topics.txt"        save_topics_to_txt(file_name, lda_topics, nmf_topics, bertopic_topics)    # Process each product_id within the product type    for product_id in df_product_type["product_id"].unique():        df_product = df_product_type[df_product_type["product_id"] == product_id]        if not df_product.empty:            print(f"Processing {product_type} - {product_id} at product level...")            texts = df_product["content"].tolist()            lda_topics = run_lda(texts)            nmf_topics = run_nmf(texts)            bertopic_topics = run_bertopic(texts)            file_name = f"{product_type.replace(' ', '_')}_{product_id}_Topics.txt"            save_topics_to_txt(file_name, lda_topics, nmf_topics, bertopic_topics)print("✅ Topic modeling completed successfully.")